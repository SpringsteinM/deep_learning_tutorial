{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKN7El7YT44Mw/mLbs/sPB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SpringsteinM/deep_learning_tutorial/blob/main/summer_school_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beispiel für die Extraktion von Merkmalen anhand eines trainierten Modells\n",
        "\n",
        "Dabei verwenden wir das Modell CLIP, das aus einem visuellen und einem textuellen Modell besteht, um Merkmalsvektoren für einen Satz von Bildern zu erzeugen."
      ],
      "metadata": {
        "id": "6Lp0EUqV59Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Herunterladen des \"Painter by Numbers\" Kunstdatensatzes"
      ],
      "metadata": {
        "id": "xeF7WbE7qmQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle_key = \"KAGGLE\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "2reJLhNkzmkO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGjqI4g_2qGs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install open_clip_torch;\n",
        "!pip install kaggle;\n",
        "!pip install torchdata\n",
        "!pip install pillow\n",
        "!mkdir ~/.kaggle -p;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "kaggle_path = str(Path.home()/\".kaggle\"/\"kaggle.json\")\n",
        "with open(kaggle_path,\"w\") as f:\n",
        "  json.dump({\"username\":\"bhaallord\",\"key\":kaggle_key},f)"
      ],
      "metadata": {
        "id": "kboMs5wnlct-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c painter-by-numbers -f train_1.zip"
      ],
      "metadata": {
        "id": "9m2kerGwmNFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "import io\n",
        "import zipfile\n",
        "import csv\n",
        "from torchdata.datapipes.iter import FileLister, FileOpener\n",
        "import imageio\n",
        "import numpy as np\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n"
      ],
      "metadata": {
        "id": "nMQLGVGYkZBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_iterator(data_path=\"train_1.zip\"):\n",
        "  \n",
        "    dp = FileLister(\".\", \"*.zip\")\n",
        "    dp = FileOpener(dp, mode=\"b\")\n",
        "    dp = dp.load_from_zip()\n",
        "\n",
        "    def decode_data(d):\n",
        "      try:\n",
        "        filename = d[0].split(\"/\")[-1]\n",
        "        return {\"filename\": filename, \"image_data\": d[1]}\n",
        "      except:\n",
        "        return None\n",
        "\n",
        "    dp = dp.map(decode_data)\n",
        "\n",
        "    def decode_image(d):\n",
        "      try:\n",
        "        filename = d[\"filename\"]\n",
        "        image = imageio.imread(d[\"image_data\"])\n",
        "        if len(image.shape) == 2:\n",
        "          image = np.expand_dims(image, axis=-1)\n",
        "        if image.shape[2] != 3:\n",
        "          image = np.concatenate([image,image,image], axis=-1)\n",
        "        if image.shape[-1] != 3:\n",
        "          return None\n",
        "        return {\"filename\": filename, \"image\": image}\n",
        "      except:\n",
        "        return None\n",
        "\n",
        "    dp = dp.map(decode_image)\n",
        "    dp = dp.filter(lambda x: x is not None)\n",
        "\n",
        "    dp = dp.map(lambda x: {**x, \"image\": preprocess(to_pil_image(x[\"image\"]))})\n",
        "    \n",
        "    return DataLoader(dp, batch_size=32, num_workers=1)\n",
        "\n",
        "def get_image(filename=\"1.jpg\", data_path=\"train_1.zip\"):\n",
        "      with zipfile.ZipFile(data_path, \"r\") as zip:\n",
        "        with zip.open(f\"train_1/{filename}\") as file:\n",
        "          return imageio.imread(file)"
      ],
      "metadata": {
        "id": "d5WFD-VWRqJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = 500 #@param {type:\"slider\", min:100, max:20000, step:1}"
      ],
      "metadata": {
        "id": "bDaNt8QGXVV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "image_embeddings = []\n",
        "image_filenames = []\n",
        "count = 0\n",
        "for  x in tqdm(generate_iterator()):\n",
        "  with torch.no_grad():\n",
        "    count +=  x[\"image\"].shape[0]\n",
        "    image_feature = model.encode_image(x[\"image\"])\n",
        "    image_filenames.extend(x[\"filename\"])\n",
        "    image_embeddings.append(image_feature)\n",
        "    if count > num_images:\n",
        "      break\n",
        "\n",
        "image_embeddings = np.concatenate(image_embeddings, axis=0)"
      ],
      "metadata": {
        "id": "kwuGGrAE4B7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_embeddings.shape)"
      ],
      "metadata": {
        "id": "ZPjgx9g1YZh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suchen nach ähnlichen Bildern\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-CZh44yGZMSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Klg7pUVnZMsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "for k, v in uploaded.items():\n",
        "  image = preprocess(to_pil_image(imageio.imread(v)[...,0:3])).unsqueeze(0)\n",
        "  with torch.no_grad():\n",
        "    image_feature = model.encode_image(image)\n",
        "  sim = cosine_similarity(image_feature, image_embeddings)\n",
        "  values, indices = torch.topk(torch.from_numpy(sim), 5)\n",
        "  \n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  axs.imshow(imageio.imread(v)[...,0:3])\n",
        "  plt.title(f\"query\")\n",
        "  for i, index in enumerate(indices.squeeze().numpy().tolist()):\n",
        "    \n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "    axs.imshow(get_image(image_filenames[index]))\n",
        "    plt.title(f\"similarity {values[0,i]}\")\n"
      ],
      "metadata": {
        "id": "6N99RCOQZZ5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suchen nach Bildern mithilfe von Textbeschreibungen\n",
        "\n"
      ],
      "metadata": {
        "id": "sDIpjg3Osq07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'an image of a woman' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "JsG11iCZs1p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open_clip.tokenize([text])\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text)\n",
        "    \n",
        "    sim = cosine_similarity(text_features, image_embeddings)\n",
        "    values, indices = torch.topk(torch.from_numpy(sim), 5)\n",
        "    \n",
        "    for i, index in enumerate(indices.squeeze().numpy().tolist()):\n",
        "      \n",
        "      fig, axs = plt.subplots(1, 1)\n",
        "      axs.imshow(get_image(image_filenames[index]))\n",
        "      plt.title(f\"similarity {values[0,i]}\")\n"
      ],
      "metadata": {
        "id": "BsD1Z23MWGhJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}